{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hr2tPjlrf-rP"
   },
   "source": [
    "# Practice Session 1: Policy and Value Iteration\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/124XxIn0ML8XslcPs4Cnwimab-EuFW8D6#scrollTo=Hr2tPjlrf-rP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHgub_3DH-9V"
   },
   "source": [
    "## Setting up the RL task with Gymnasium\n",
    "\n",
    "Gymnasium (https://gymnasium.farama.org/) is a standard interface for simulating Markov Decision Processes (MDPs) based on OpenAI's Gym library.\n",
    "\n",
    "An MDP is a modelling tool for Reinforcement Learning tasks. It comprises of:\n",
    "* a finite **state space** $\\mathcal{S}$\n",
    "* a finite **action space** $\\mathcal{A}$\n",
    "* a **starting-state distribution** $\\nu_0(s)$\n",
    "* a **reward function** $r: \\mathcal{S}\\times\\mathcal{A}\\rightarrow [0,1]$ (or vector $r\\in[0,1]^{|\\mathcal{S}|\\cdot|\\mathcal{A}|}$)\n",
    "* a **transition function** $p: \\mathcal{S}\\times\\mathcal{A}\\rightarrow \\Delta_{\\mathcal{S}}$ (or matrix $p \\in\\mathbb{R}^{|\\mathcal{S}|\\cdot|\\mathcal{A}|\\times|\\mathcal{S}|}$)\n",
    "\n",
    "<div>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=11KyhHxuaileBEJ9fFZmIS5-GzncQgyqt\" alt=\"agent-environment interaction\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POptscUmHhZY"
   },
   "source": [
    "### Minigrid Environment\n",
    "Using the Minigrid (https://minigrid.farama.org/) package we develop a 2D grid world environment with explicit transition and reward functions.\n",
    "\n",
    "The following code is meant to:\n",
    "*   install configured minigrid package\n",
    "*   import the minigrid package and other helpful packages\n",
    "*   instantiate an $n$ by $n$ grid world environment with random seed to make the experiment fully reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24916,
     "status": "ok",
     "timestamp": 1687025588827,
     "user": {
      "displayName": "NNEKA MAUREEN OKOLO",
      "userId": "04264045196225705265"
     },
     "user_tz": -120
    },
    "id": "VYZ_hFDkXuY0",
    "outputId": "7e64824c-4383-4b8c-8ff0-9aa5c00613c2"
   },
   "outputs": [],
   "source": [
    "pip install \"git+https://github.com/cipollone/rlss-practice1.git\" --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 353,
     "status": "ok",
     "timestamp": 1687025589173,
     "user": {
      "displayName": "NNEKA MAUREEN OKOLO",
      "userId": "04264045196225705265"
     },
     "user_tz": -120
    },
    "id": "XfYfq9HebRHw",
    "outputId": "442e028e-7e7f-4c06-aeb4-33aa7555168a"
   },
   "outputs": [],
   "source": [
    "from rlss_practice.environments import Room\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1687025589173,
     "user": {
      "displayName": "NNEKA MAUREEN OKOLO",
      "userId": "04264045196225705265"
     },
     "user_tz": -120
    },
    "id": "YMh2GonKaiOk"
   },
   "outputs": [],
   "source": [
    "n = 5\n",
    "# seed = 4\n",
    "\n",
    "# instantiate 5 by 5 grid with 0.1 failure probability\n",
    "# env = Room(seed=seed, failure=0.1, size=n)\n",
    "env = Room(failure=0.1, size=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdPVJfXpHoJD"
   },
   "source": [
    "#### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1687025589173,
     "user": {
      "displayName": "NNEKA MAUREEN OKOLO",
      "userId": "04264045196225705265"
     },
     "user_tz": -120
    },
    "id": "LifZ-NV5XtGC"
   },
   "outputs": [],
   "source": [
    "# some demo or exercise to get familiar with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1687025589174,
     "user": {
      "displayName": "NNEKA MAUREEN OKOLO",
      "userId": "04264045196225705265"
     },
     "user_tz": -120
    },
    "id": "51HZLSR-YV_Q"
   },
   "outputs": [],
   "source": [
    "# checkout env.StateT to get structure of states\n",
    "# explain what each entry of the state tuple refers to\n",
    "# checkout len(env.states) to see the number of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1687025589583,
     "user": {
      "displayName": "NNEKA MAUREEN OKOLO",
      "userId": "04264045196225705265"
     },
     "user_tz": -120
    },
    "id": "3NDT8Lv3YsjH"
   },
   "outputs": [],
   "source": [
    "# actions are int between 0 and 2.\n",
    "# describe what each action refers to\n",
    "# checkout num of actions with env.action_space.n\n",
    "# access actions with env.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1687025589584,
     "user": {
      "displayName": "NNEKA MAUREEN OKOLO",
      "userId": "04264045196225705265"
     },
     "user_tz": -120
    },
    "id": "CE-TQiXSbL6K"
   },
   "outputs": [],
   "source": [
    "# checkout transition and reward\n",
    "# run agent with random policy to see how it interacts with the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8L7zlYG8WcPq"
   },
   "source": [
    "## Solving the Grid-world task\n",
    "\n",
    "We would like to find a **stationary deterministic memoryless policy** or mapping from states to actions $\\pi : \\mathcal{S}\\rightarrow \\mathcal{A}$, that lets us arrive at the **goal state** sooner.\n",
    "\n",
    "In other words, our objective to find $\\pi$ which maximizes the **discounted return** from any starting state represented by:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\rho(\\pi) &= \\mathbb{E}_{s_0\\sim\\nu_0, s_{t+1}\\sim p(\\cdot|s_t,\\pi(s_t))}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t,\\pi(s_t))\\right]\\\\\n",
    "    &= \\sum_{s}\\nu_0(s) V^{\\pi}(s)\n",
    "\\end{align*}\n",
    "\n",
    "with the **discount factor** $\\gamma\\in[0,1)$.\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "---\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Recall that the **state value function** for $s\\in \\mathcal{S}$ denoted\n",
    "\n",
    "\\begin{align*}\n",
    "    V^\\pi(s) &= \\mathbb{E}_{s_{t+1}\\sim p(\\cdot|s_t,\\pi(s_t))}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t,\\pi(s_t))\\bigg|s_0=s\\right]\\\\\n",
    "    &=  \\sum_{a}\\pi(a|s)Q^{\\pi}(s,a)\n",
    "\\end{align*}\n",
    "\n",
    "represents the value of being in state $x$ and following policy $\\pi$ while the **action-value function** for $x\\in \\mathcal{S}, a\\in \\mathcal{A}$ denoted\n",
    "\n",
    "$$ Q^\\pi(s,a) = r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V^{\\pi}(s')$$\n",
    "\n",
    "is the value of first taking action $a=\\pi(s)$ in state $s$ then following policy $\\pi$.\n",
    "\n",
    "By standard notation, the **Bellman operator** of policy $\\pi$ acting on functions $V : \\mathcal{S}\\rightarrow \\mathbb{R}$ (or vectors $V\\in\\mathbb{R}^{|\\mathcal{S}|}$) is given as:\n",
    "\n",
    "$$(T^{\\pi}V)(s) =  \\sum_{a}\\pi(a|s)\\bigg[r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V(s')\\bigg],\\quad s\\in\\mathcal{S}$$\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "---\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Ideally, we would like to find an **optimal policy** $\\pi^*$ with:\n",
    "\n",
    "$$ \\pi^*(s) \\in \\underset{a\\in\\mathcal{A}}{\\arg\\max} \\bigg\\{r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V^{\\pi^*}(s')\\bigg\\},\\quad x\\in\\mathcal{S}$$\n",
    "\n",
    "that maximizes our immediate reward and future return.\n",
    "\n",
    "The Bellman operator of $\\pi^*$ (a.k.a the **Bellman optimality operator**) acting on functions $V : \\mathcal{S}\\rightarrow \\mathbb{R}$ (or vectors $V\\in\\mathbb{R}^{|\\mathcal{S}|}$) is given as:\n",
    "\n",
    "$$(T^{*}V)(s) =  \\max_{a}\\bigg\\{r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V(s')\\bigg\\},\\quad s\\in\\mathcal{S}$$\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "---\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "With the transition and reward function of the grid-world task available to us, we attempt to find an optimal policy via **Dynamic programming**. Precisely, we implement **Policy Iteration** and **Value iteration** methods introduced in the first lecture.\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nljnBe4bZTf"
   },
   "source": [
    "### Policy Iteration (PI) Recap\n",
    "\n",
    "**Idea**\n",
    "\n",
    "Gradually advance to $\\pi^{*}$ from an initial guess $\\pi_0$ through a series of **policy evaluation** and **policy improvement** steps.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Policy Evaluation step**\n",
    "\n",
    "Given a policy $\\pi_k$, compute $V^{\\pi_k}$ as\n",
    "\n",
    "* $V^{\\pi_k} = (I - \\gamma p_{\\pi_k})^{-1}r_{\\pi_k}$\n",
    "\n",
    "or estimate $V^{\\pi_k}$ as $V_{k}$ with the following iterations\n",
    "\n",
    "  * Initialize $V$ as $\\mathbf{0}$ when $k = 0$ and $V_{k-1}$ otherwise, let $\\pi = \\pi_k$\n",
    "\n",
    "    <div>\n",
    "    <img src=\"https://drive.google.com/uc?export=view&id=1QaMg7a6HELjYycAnm6RE9vnzHD_fnaCn\" alt=\"iterative policy evaluation\" width=\"600\"/>\n",
    "    </div>\n",
    "\n",
    "    Return $V_k = V$. <!--$(T^{\\pi_{k}})^{n_{k}}V_{k-1}$-->\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Policy Improvement step**\n",
    "\n",
    "Obtain $\\pi_{k+1}$ as the greedy policy w.r.t $V^{\\pi_k}$ (or $V_{k}$). That is,\n",
    "$$ \\pi_{k+1}(s) \\in \\underset{a\\in\\mathcal{A}}{\\arg\\max} \\bigg\\{r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{X}}p(s'|s,a)V^{\\pi_{k}}(s')\\bigg\\},\\quad s\\in\\mathcal{S}$$\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "#### Putting everything together\n",
    "\n",
    "Starting from an arbitrary stationary deterministic markovian policy $\\pi_{0}$, for $k = 0,1,2,\\cdots, K$ do:\n",
    "* Compute $V^{\\pi_k}$ or estimate $V^{\\pi_k}$ with $V_{k}=(T^{\\pi_{k}})^{n_{k}}V_{k-1}$\n",
    "* Obtain $\\pi_{k+1}$ as the greedy policy w.r.t $V^{\\pi_k}$\n",
    "\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "#### Theoretical guarantee\n",
    "\n",
    "PI with \"exact\" policy evaluation steps finds an optimal policy after $K = \\mathcal{\\tilde{O}}((\\mathcal{SA - S})/(1-\\gamma))$ iterations.\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "#### Notation\n",
    "\n",
    "*   $r_{\\pi}\\in\\mathbb{R}^{|\\mathcal{S}|}$ so that for $s\\in\\mathcal{S}$, $r_{\\pi}(s) = r(s,\\pi(s))$.\n",
    "*   $p_{\\pi}\\in\\mathbb{R}^{|\\mathcal{S}|\\times|\\mathcal{S}|}$ so that for $s,s'\\in\\mathcal{S}$, $p_{\\pi}(s'|s) = p(s'|s,\\pi(s))$.\n",
    "*   $n_{k}$ is the number of loops required to compute $V_{k}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1687025589584,
     "user": {
      "displayName": "NNEKA MAUREEN OKOLO",
      "userId": "04264045196225705265"
     },
     "user_tz": -120
    },
    "id": "Woh49RAfbieB"
   },
   "outputs": [],
   "source": [
    "class PolicyIteration1:\n",
    "    \"\"\"\n",
    "    Implements policy iteration with exact policy evaluation\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                discount_factor: float,\n",
    "                initial_policy = None,\n",
    "                fig = None,\n",
    "                ax = None) -> None:\n",
    "      self.gamma = discount_factor\n",
    "      self.policy = initial_policy\n",
    "\n",
    "      if self.policy == None:\n",
    "        np.random.seed(4)\n",
    "        self.policy = {state: np.random.randint(0,env.action_space.n) for state in env.states}\n",
    "\n",
    "      self.n_states = len(env.states)\n",
    "      self.V = np.zeros((self.n_states,1))\n",
    "      self.policy_stable = False\n",
    "      self.max_value_gap = np.inf\n",
    "      self.fig = fig\n",
    "      self.ax = ax\n",
    "      self.count = 0\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate_policy(self):\n",
    "      \"\"\"\n",
    "      Given π_{k} compute V^{π_{k}} = (I - \\gamma p_{π_{k}})^{-1}r_{π_{k}}.\n",
    "      \"\"\"\n",
    "\n",
    "      plt.title('PI1: value estimate of π_'+str(self.count))\n",
    "\n",
    "      self.V = np.linalg.inv(np.eye(self.n_states,self.n_states) - self.gamma*self.get_p_pi())\n",
    "      self.V = self.V@self.get_r_pi()\n",
    "\n",
    "      # ----- plot state values -----\n",
    "\n",
    "      self.ax.bar(range(len(env.states)),\n",
    "                  self.V.flatten(),\n",
    "                  color = '#1f77b4')\n",
    "      clear_output(wait=True)\n",
    "      display(self.fig)\n",
    "      self.count +=1\n",
    "      return\n",
    "\n",
    "\n",
    "    def get_policy(self):\n",
    "      \"\"\"\n",
    "      Compute the greedy policy:\n",
    "        π_{k+1}(s) = argmax_{a\\in A} Q^{\\pi_{k}}(s,a)\n",
    "      where\n",
    "        Q^{\\pi_{k}}(s,a) = R(s,a) + gamma*<P(.|s,a),V^{\\pi_{k}}>\n",
    "      is the state-action value.\n",
    "      \"\"\"\n",
    "      self.policy_stable = True\n",
    "\n",
    "      for state in env.states:\n",
    "        max_Q_value = self.get_expected_update(state, self.policy[state])\n",
    "\n",
    "        for action in env.actions:\n",
    "          Q_value = self.get_expected_update(state, action)\n",
    "\n",
    "          if action != self.policy[state] and max_Q_value<Q_value:\n",
    "            self.policy[state] = action\n",
    "            max_Q_value = Q_value\n",
    "            self.policy_stable = False\n",
    "      return\n",
    "\n",
    "\n",
    "    def get_expected_update(self, state, action):\n",
    "      \"\"\"\n",
    "      Compute Bellman update at a state-action pair.\n",
    "\n",
    "      input:\n",
    "      state\n",
    "      action\n",
    "      discount factor (gamma)\n",
    "      state values (state_values)\n",
    "\n",
    "      output:\n",
    "      r(s,a) + gamma <P(.|s,a),v>\n",
    "      \"\"\"\n",
    "      value  = env.R[state][action]\n",
    "      snext_index = 0\n",
    "\n",
    "      for snext in env.states:\n",
    "        value += self.gamma*env.T[state][action][snext]*self.V[snext_index]\n",
    "        snext_index += 1\n",
    "\n",
    "      return value\n",
    "\n",
    "\n",
    "    def get_p_pi(self):\n",
    "      \"\"\"\n",
    "      Given π_{k}, compute p_{π_{k}}\n",
    "      \"\"\"\n",
    "      p_pi = np.zeros((self.n_states,self.n_states))\n",
    "      s_index,snext_index = (0,0)\n",
    "\n",
    "      for s in env.states:\n",
    "        for snext in env.states:\n",
    "          p_pi[s_index,snext_index] = env.T[s][self.policy[s]][snext]\n",
    "          snext_index += 1\n",
    "        snext_index=0\n",
    "        s_index += 1\n",
    "\n",
    "      return p_pi\n",
    "\n",
    "\n",
    "    def get_r_pi(self):\n",
    "      \"\"\"\n",
    "      Given π_{k}, compute r_{π_{k}}\n",
    "      \"\"\"\n",
    "      r_pi = np.zeros((self.n_states,1))\n",
    "      s_index = 0\n",
    "\n",
    "      for s in env.states:\n",
    "        r_pi[s_index][0] = env.R[s][self.policy[s]]\n",
    "        s_index+=1\n",
    "\n",
    "      return r_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1687025589585,
     "user": {
      "displayName": "NNEKA MAUREEN OKOLO",
      "userId": "04264045196225705265"
     },
     "user_tz": -120
    },
    "id": "nFkMVMI3syvt"
   },
   "outputs": [],
   "source": [
    "class PolicyIteration2:\n",
    "    \"\"\"\n",
    "    Implements policy iteration with iterative policy evaluation\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                discount_factor: float,\n",
    "                theta: float,\n",
    "                initial_policy = None,\n",
    "                fig = None,\n",
    "                ax = None) -> None:\n",
    "      self.gamma = discount_factor\n",
    "      self.theta = theta\n",
    "      self.policy = initial_policy\n",
    "\n",
    "      if self.policy == None:\n",
    "        np.random.seed(4)\n",
    "        self.policy = {state: np.random.randint(0,env.action_space.n) for state in env.states}\n",
    "\n",
    "      self.V = {state: 0 for state in env.states}\n",
    "      self.policy_stable = False\n",
    "      self.max_value_gap = np.inf\n",
    "      self.fig = fig\n",
    "      self.ax = ax\n",
    "      self.count = 0\n",
    "\n",
    "\n",
    "    def evaluate_policy(self):\n",
    "      \"\"\"\n",
    "      Starting from previous value estimate V_{k-1}\n",
    "      estimate value of policy π_{k} by recursively applying\n",
    "      the Bellman operator of the policy T^π_{k} to V_{k-1} until update is stable.\n",
    "      \"\"\"\n",
    "      plt.title('PI2: value estimate of π_'+str(self.count))\n",
    "\n",
    "      while self.max_value_gap > self.theta:\n",
    "        self.max_value_gap = 0\n",
    "        counter = 0\n",
    "\n",
    "        for state in env.states:\n",
    "          prev_statevalue = self.V[state]\n",
    "          self.V[state] = self.get_expected_update(state, self.policy[state])\n",
    "          self.max_value_gap = max(self.max_value_gap, np.abs(prev_statevalue - self.V[state]))\n",
    "\n",
    "        # ----- plot state values -----\n",
    "        if counter%10000==0:\n",
    "          self.ax.bar(range(len(env.states)),\n",
    "                      list(map(lambda state: self.V[state],env.states)),\n",
    "                      color = '#DC143C')\n",
    "          clear_output(wait=True)\n",
    "          display(self.fig)\n",
    "        counter+=1\n",
    "\n",
    "      self.max_value_gap = np.inf\n",
    "      self.count +=1\n",
    "      return\n",
    "\n",
    "\n",
    "    def get_policy(self):\n",
    "      \"\"\"\n",
    "      Update policy to be the greedy policy:\n",
    "        π(s) = argmax_{a\\in A} Q(s,a)\n",
    "      where\n",
    "        Q(s,a) = R(s,a) + gamma*<P(.|s,a),v>\n",
    "      is the state-action value.\n",
    "      \"\"\"\n",
    "      self.policy_stable = True\n",
    "\n",
    "      for state in env.states:\n",
    "        max_Q_value = self.get_expected_update(state, self.policy[state])\n",
    "\n",
    "        for action in env.actions:\n",
    "          Q_value = self.get_expected_update(state, action)\n",
    "\n",
    "          if action != self.policy[state] and max_Q_value<Q_value:\n",
    "            self.policy[state] = action\n",
    "            max_Q_value = Q_value\n",
    "            self.policy_stable = False\n",
    "      return\n",
    "\n",
    "\n",
    "    def get_expected_update(self, state, action):\n",
    "      \"\"\"\n",
    "      Compute Bellman update at a state-action pair.\n",
    "\n",
    "      input:\n",
    "      state\n",
    "      action\n",
    "      discount factor (gamma)\n",
    "      state values (state_values)\n",
    "\n",
    "      output:\n",
    "      r(s,a) + gamma <P(.|s,a),v>\n",
    "      \"\"\"\n",
    "      value  = env.R[state][action]\n",
    "\n",
    "      for snext in env.states:\n",
    "        value += self.gamma*env.T[state][action][snext]*self.V[snext]\n",
    "\n",
    "      return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IA8AkX2Feuwt"
   },
   "source": [
    "### Value Iteration (VI) Recap\n",
    "\n",
    "**Idea**\n",
    "\n",
    "Gradually advance to $\\pi^{*}$ with a single **iterative policy evaluation** step and **policy improvement**.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Policy Evaluation step**\n",
    "\n",
    "No arbitrary starting policy needed. Estimate $V^{\\pi^*}$ as follows:\n",
    "\n",
    "  * For $k = 0,1,2,\\cdots, K$\n",
    "\n",
    "    * Set $V = \\mathbf{0}$ when $k = 0$ and $V_{k-1}$ otherwise\n",
    "      <div>\n",
    "      <img src=\"https://drive.google.com/uc?export=view&id=1fsi3ZZgqZ-p061AxvSdeluWZTlwSjnGj\" alt=\"iterative policy evaluation\" width=\"400\"/>\n",
    "      </div>\n",
    "\n",
    "      return $V$ as $V_{k}$.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Policy Improvement step**\n",
    "\n",
    "Obtain $\\hat{\\pi}^{*}$ as the greedy policy w.r.t $V_{K}$. That is,\n",
    "\n",
    "  $$ \\hat{\\pi}^{*}(s) \\in \\underset{a\\in\\mathcal{A}}{\\arg\\max} \\bigg\\{r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{X}}p(s'|s,a)V_{K}(s')\\bigg\\},\\quad s\\in\\mathcal{S}$$\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "#### Putting everything together\n",
    "\n",
    "* Estimate $V^{\\pi^*}$ with $V_{K} = (T^*)^{K}\\mathbf{0}$\n",
    "* Obtain $\\hat{\\pi}^{*}$ as the greedy policy w.r.t $V_{K}$\n",
    "\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "#### Theoretical guarantee\n",
    "\n",
    "VI finds an **$\\mathbf{\\varepsilon}$-optimal policy** ($\\pi^{\\varepsilon}$) satisfying\n",
    "\n",
    "$\\qquad V^* - V^{\\pi^{\\varepsilon}}\\leq \\varepsilon e$\n",
    "\n",
    "after $K = \\mathcal{O}(\\ln(2\\gamma/\\varepsilon(1-\\gamma)^2)/(1-\\gamma))$ iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1687025589585,
     "user": {
      "displayName": "NNEKA MAUREEN OKOLO",
      "userId": "04264045196225705265"
     },
     "user_tz": -120
    },
    "id": "cWtqSTiD9Kko"
   },
   "outputs": [],
   "source": [
    "class ValueIteration:\n",
    "    \"\"\"\n",
    "    Implements value iteration\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                discount_factor: float,\n",
    "                epsilon: float,\n",
    "                num_iterations = None,\n",
    "                fig = None,\n",
    "                ax = None) -> None:\n",
    "      self.gamma = discount_factor\n",
    "      self.K = num_iterations\n",
    "\n",
    "      if self.K == None:\n",
    "        self.K = math.ceil(np.log((2*self.gamma)/(epsilon*(1-self.gamma)**2))/(1-self.gamma))\n",
    "\n",
    "      self.policy = {state: 0 for state in env.states}\n",
    "\n",
    "      self.V = {state: 0 for state in env.states}\n",
    "      self.fig = fig\n",
    "      self.ax = ax\n",
    "\n",
    "\n",
    "    def evaluate_policy(self):\n",
    "      \"\"\"\n",
    "      Starting from an arbitrary value V_0(s) = 0 for s in S\n",
    "      estimate the V^π* by recursively applying\n",
    "      the Bellman optimality operator of the policy T^* to V_0 for K steps.\n",
    "      \"\"\"\n",
    "\n",
    "      for k in range(self.K):\n",
    "        plt.title('VI: value estimate of π* (V_'+str(k)+\")\")\n",
    "        for state in env.states:\n",
    "          self.V[state] = max(map(lambda action: self.get_expected_update(state, action), env.actions))\n",
    "\n",
    "        # ----- plot state values -----\n",
    "\n",
    "        if k%10==0:\n",
    "          self.ax.bar(range(len(env.states)),\n",
    "                      list(map(lambda state: self.V[state],env.states)),\n",
    "                      color = '#008080')\n",
    "          clear_output(wait=True)\n",
    "          display(self.fig)\n",
    "      return\n",
    "\n",
    "\n",
    "    def get_policy(self):\n",
    "      \"\"\"\n",
    "      Update policy to be the greedy policy:\n",
    "        π(s) = argmax_{a\\in A} Q(s,a)\n",
    "      where\n",
    "        Q(s,a) = R(s,a) + gamma*<P(.|s,a),v>\n",
    "      is the state-action value.\n",
    "      \"\"\"\n",
    "      for state in env.states:\n",
    "        max_Q_value = self.get_expected_update(state, self.policy[state])\n",
    "        for action in env.actions:\n",
    "          Q_value = self.get_expected_update(state, action)\n",
    "          if action != self.policy[state] and max_Q_value<Q_value:\n",
    "            self.policy[state] = action\n",
    "            max_Q_value = Q_value\n",
    "      return\n",
    "\n",
    "\n",
    "    def get_expected_update(self, state, action):\n",
    "      \"\"\"\n",
    "      Compute Bellman update at a state-action pair.\n",
    "\n",
    "      input:\n",
    "      state\n",
    "      action\n",
    "      discount factor (gamma)\n",
    "      state values (state_values)\n",
    "\n",
    "      output:\n",
    "      r(s,a) + gamma <P(.|s,a),v>\n",
    "      \"\"\"\n",
    "      value  = env.R[state][action]\n",
    "\n",
    "      for snext in env.states:\n",
    "        value += self.gamma*env.T[state][action][snext]*self.V[snext]\n",
    "\n",
    "      return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nuH15i1W4qr"
   },
   "source": [
    "### Run Policy and Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1687025589585,
     "user": {
      "displayName": "NNEKA MAUREEN OKOLO",
      "userId": "04264045196225705265"
     },
     "user_tz": -120
    },
    "id": "yyFqB2DG5Alo"
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    discount_factor = 0.99\n",
    "    epsilon = 0.01\n",
    "    theta = 0.1\n",
    "\n",
    "    mpl.style.use('seaborn-v0_8')\n",
    "\n",
    "\n",
    "    #run policy iteration with exact policy evaluation step\n",
    "    fig1 = plt.figure()\n",
    "    ax1 = plt.gca()\n",
    "    plt.ylabel(\"values\")\n",
    "    plt.xlabel(\"states\")\n",
    "\n",
    "    PI_planner1 = PolicyIteration1(discount_factor,\n",
    "                                   fig = fig1,\n",
    "                                   ax = ax1)\n",
    "\n",
    "    while not PI_planner1.policy_stable:\n",
    "      PI_planner1.evaluate_policy()\n",
    "      PI_planner1.get_policy()\n",
    "\n",
    "    #plt.close()\n",
    "\n",
    "\n",
    "    #run policy iteration with iterative policy evaluation step\n",
    "    fig2 = plt.figure()\n",
    "    ax2 = plt.gca()\n",
    "    plt.ylabel(\"values\")\n",
    "    plt.xlabel(\"states\")\n",
    "\n",
    "    PI_planner2 = PolicyIteration2(discount_factor,\n",
    "                                   theta,\n",
    "                                   fig = fig2,\n",
    "                                   ax = ax2)\n",
    "\n",
    "    while not PI_planner2.policy_stable:\n",
    "      PI_planner2.evaluate_policy()\n",
    "      PI_planner2.get_policy()\n",
    "\n",
    "    #plt.close()\n",
    "\n",
    "\n",
    "    # run value iteration\n",
    "    fig3 = plt.figure()\n",
    "    ax3 = plt.gca()\n",
    "    plt.ylabel(\"values\")\n",
    "    plt.xlabel(\"states\")\n",
    "\n",
    "    VI_planner = ValueIteration(discount_factor,\n",
    "                                epsilon,\n",
    "                                fig = fig3,\n",
    "                                ax = ax3)\n",
    "\n",
    "    VI_planner.evaluate_policy()\n",
    "    VI_planner.get_policy()\n",
    "\n",
    "    #plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1100208,
     "status": "ok",
     "timestamp": 1687026689785,
     "user": {
      "displayName": "NNEKA MAUREEN OKOLO",
      "userId": "04264045196225705265"
     },
     "user_tz": -120
    },
    "id": "SqvEkb47-dDH",
    "outputId": "8a5b5102-3993-47f8-b996-1077071b5805"
   },
   "outputs": [],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CWgA6ZjQ6KK"
   },
   "source": [
    "### Test Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1687026689785,
     "user": {
      "displayName": "NNEKA MAUREEN OKOLO",
      "userId": "04264045196225705265"
     },
     "user_tz": -120
    },
    "id": "dgj9qrYohLWm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqJPFe6f6gFV"
   },
   "source": [
    "Credit:\n",
    "\n",
    "*   Bruno Scherrer, \"a lecture on Markov Decision Processes and Dynamic Programming\", June 2023, [link text](https://)\n",
    "*   Csaba Szepesvári \"a lecture series on Theoretical Foundations of Reinforcement Learning\", 2020, [RL Theory course](https://rltheory.github.io/)\n",
    "*   Richard S. Sutton, Andrew G. Barto \"Reinforcement Learning: An Introduction\", second edition, 2020, [link text](http://incompleteideas.net/book/RLbook2020.pdf)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1687026689786,
     "user": {
      "displayName": "NNEKA MAUREEN OKOLO",
      "userId": "04264045196225705265"
     },
     "user_tz": -120
    },
    "id": "Pzk9N77thKNi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
