{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hr2tPjlrf-rP"
   },
   "source": [
    "# Practice Session 1: Policy and Value Iteration\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rlsummerschool/practical-sessions/blob/master/notebooks/Practice_1.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VYZ_hFDkXuY0",
    "outputId": "20b8e3d7-38cc-4014-bb7d-b7baa5f9acdf"
   },
   "outputs": [],
   "source": [
    "# Set up the environment\n",
    "# TODO: this will point to rlsummerschool's repo, once it's made bublic\n",
    "!pip install \"git+https://github.com/cipollone/rlss-practice1.git\" --quiet\n",
    "\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFtCdzEI9gHZ"
   },
   "source": [
    "## RL development in Python\n",
    "\n",
    "Large RL projects are usually developed locally, as any other Python package, in OOP, version controlled by Git (see [example](https://github.com/rlsummerschool/practical-sessions/blob/master/rlss_practice/environments.py)).\n",
    "\n",
    "[Jupyter](https://jupyter.org/) notebooks, like this one, are collections of text cells and code cells, that can be executed with an interactive interpreter.\n",
    "Colab notebooks are Jupyter notebooks whose interpreter runs in a runtime that is hosted by Google."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqPq-DrFG3se"
   },
   "source": [
    "### Environments and tasks\n",
    "\n",
    "An MDP is a model of Reinforcement Learning tasks. It comprises:\n",
    "* a finite **state space** $\\mathcal{S}$\n",
    "* a finite **action space** $\\mathcal{A}$\n",
    "* a **starting-state distribution** $\\nu_0(s)$\n",
    "* a **reward function** $r: \\mathcal{S}\\times\\mathcal{A}\\rightarrow [0,1]$ (or vector $r\\in[0,1]^{|\\mathcal{S}|\\cdot|\\mathcal{A}|}$)\n",
    "* a **transition function** $p: \\mathcal{S}\\times\\mathcal{A}\\rightarrow \\Delta_{\\mathcal{S}}$ (or matrix $p \\in\\mathbb{R}^{|\\mathcal{S}|\\cdot|\\mathcal{A}|\\times|\\mathcal{S}|}$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsQticuSJmdO"
   },
   "source": [
    "<div>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=11KyhHxuaileBEJ9fFZmIS5-GzncQgyqt\" alt=\"agent-environment interaction\" width=\"600\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ob8WriuZLh-g"
   },
   "source": [
    "### The Gym(nasium) interface\n",
    "\n",
    "[Gymnasium](https://gymnasium.farama.org/) is a standard API for Decision Processes, based on OpenAI's Gym library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JgaqoJNTOi6o",
    "outputId": "13317af7-1da2-4e67-fba8-107bbe2c19ca"
   },
   "outputs": [],
   "source": [
    "# Standard import\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDwwk5BL1eRQ"
   },
   "source": [
    "The library also defines a number of classic benchmarks [Atari](https://gymnasium.farama.org/environments/atari/) games, [MuJoCo](https://gymnasium.farama.org/environments/mujoco/) simulations, and [ToyText](https://gymnasium.farama.org/environments/toy_text/), minimal environments for debugging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlzPIsH23hpE",
    "outputId": "6b028408-6226-4fc3-d1f7-688293f50bbf"
   },
   "outputs": [],
   "source": [
    "# A registered environment can be instantiated with\n",
    "env = gym.make(\"CliffWalking-v0\")\n",
    "\n",
    "# Important properties:\n",
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXhGGkAI7XEh"
   },
   "source": [
    "We only consider discrete state and action spaces. See the possible alternatives: `dir(gym.spaces)`\n",
    "\n",
    "Environments can be fully observable, partially, or non-stationary. All these variants fit the same environment interface.\n",
    "We should know which class our environment belongs to.\n",
    "Today, we only consider stationary MDPs (observations are states)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNcySyOGACgZ"
   },
   "source": [
    "The environment interfact allows  to sample initial states and transition with `env.reset()` and `env.step(action)`. Every method is well documented (see `help(gym.Env.step)`).\n",
    "\n",
    "Try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wnymj388egTo",
    "outputId": "5448ea89-3843-455d-f7ec-c2d50bf93259"
   },
   "outputs": [],
   "source": [
    "observation, info = env.reset()\n",
    "print(f\"Initial observation: {observation}\")\n",
    "\n",
    "action = 0  # any action\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "print(f\"Observation: {observation}, reward {reward}\")\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "print(f\"Observation: {observation}, reward {reward}\")\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "print(f\"Observation: {observation}, reward {reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xffKa7UfiOC"
   },
   "source": [
    "Custom environments can be created by subclassing the `Env` class.\n",
    "\n",
    "Environments can be also modified with wrappers. See the predefined Gymnasium [Wrappers](https://gymnasium.farama.org/api/wrappers/#gymnasium-wrappers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHgub_3DH-9V"
   },
   "source": [
    "## Setting up our RL task\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QotsgLalKv4"
   },
   "source": [
    "We exeriment with a simple grid-world environment, based on the implementation in [minigrid](https://minigrid.farama.org/environments/minigrid/). The environment is configured and observations are transformed appropriately (you already know how)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dzPxVxymjlZr",
    "outputId": "d96b443b-6f73-4af2-cde5-e8ad026622cf"
   },
   "outputs": [],
   "source": [
    "from rlss_practice.environments import Room, Rooms, MinigridBase\n",
    "\n",
    "# Initializing the environment\n",
    "env = Room(\n",
    "    failure=0.0,\n",
    "    agent_start_pos=(1, 1),\n",
    "    agent_start_dir=0,\n",
    "    size=5,\n",
    ")\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AA74pMHsl_be"
   },
   "source": [
    "In the ASCII representation, > is the agent, facing right, G is the goal, and W are walls.\n",
    "\n",
    "Understand you own environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pY4EV-n0mH-d",
    "outputId": "157fa1ca-3551-43da-da4b-80153e786d11"
   },
   "outputs": [],
   "source": [
    "print(\"Action space:\", env.action_space)\n",
    "print(\"Observation space:\", env.observation_space, end=\"\\n\\n\")\n",
    "#print(help(MinigridBase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tk3VBz-ZmsvM"
   },
   "source": [
    "    class MinigridBase\n",
    "       MinigridBase(minigrid: minigrid.minigrid_env.MiniGridEnv, seed: int, failure=0.0)\n",
    "\n",
    "       Base class for minigrid environments with explicit transition and reward functions.\n",
    "\n",
    "       The agent is rewarded upon reaching the goal location.\n",
    "\n",
    "       Action space:\n",
    "\n",
    "       | Num | Name         | Action       |\n",
    "       |-----|--------------|--------------|\n",
    "       | 0   | left         | Turn left    |\n",
    "       | 1   | right        | Turn right   |\n",
    "       | 2   | forward      | Move forward |\n",
    "\n",
    "       Observation space:\n",
    "\n",
    "       | Name | Description             |\n",
    "       |------|-------------------------|\n",
    "       | x    | x coordinate            |\n",
    "       | y    | y coordinate (downward) |\n",
    "       | dir  | cardinal direction      |\n",
    "\n",
    "       The transition function is stored in `T`,\n",
    "       where `T[state][action][next_state]` is the transition probability.\n",
    "       The reward function is `R`. `R[state][action]` contains a reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOD43HaO2M6X"
   },
   "source": [
    "### Demo\n",
    "\n",
    "Here's a rollout loop, for a single trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "ugRanvqJo4RW",
    "outputId": "e6c19886-d2ca-4e85-ecc1-5ff1de4fc602"
   },
   "outputs": [],
   "source": [
    "# Test it\n",
    "done = False\n",
    "observation, info = env.reset()\n",
    "print(\"Initial observation:\", observation)\n",
    "print(env)\n",
    "\n",
    "# Steps\n",
    "while not done:\n",
    "\n",
    "    # Action selection\n",
    "    action = int(input(\"Action: \"))\n",
    "\n",
    "    # Transition\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    print(f\"Observation: {observation}, reward {reward}\")\n",
    "    print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X910eylycXEf"
   },
   "source": [
    "We can define a function that performs `n_trajectories` rollouts on the environment with the given policy. We can also compute arbitrary statistics in the meanwhile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gU1XQ-ObDqCn"
   },
   "outputs": [],
   "source": [
    "def rollouts(env, policy, n_trajectories, gamma):\n",
    "    \"\"\"Execute policy over env for n_trajectories and compute discounted return.\"\"\"\n",
    "    total_return = 0.0\n",
    "\n",
    "    # Trajectores\n",
    "    for _ in range(n_trajectories):\n",
    "\n",
    "        # Init\n",
    "        discount = 1.0\n",
    "        ret = 0.0\n",
    "        observation, info = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # Steps\n",
    "        while not done:\n",
    "\n",
    "            # Action selection\n",
    "            action = policy(observation)\n",
    "\n",
    "            # Transition\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            done = terminated or truncated\n",
    "            ret += reward * discount\n",
    "            discount *= gamma\n",
    "\n",
    "            if done:\n",
    "                total_return += ret\n",
    "\n",
    "    env.close()\n",
    "    return total_return / n_trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmYQPLghM8cy"
   },
   "source": [
    "Since we don't have a policy, yet. Let's define the uniform one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Itk9LgVqL-DD"
   },
   "outputs": [],
   "source": [
    "class UniformPolicy():\n",
    "    def __init__(self, n_actions: int):\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def __call__(self, observation):\n",
    "        return random.randint(0, self.n_actions-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQmcdqKt31XT"
   },
   "source": [
    "Let's try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qXnOiHr234mn",
    "outputId": "4cb3b088-4d7e-4f4a-bd9d-52f9e105cc69"
   },
   "outputs": [],
   "source": [
    "avg_return = rollouts(\n",
    "    env=env,\n",
    "    policy=UniformPolicy(env.action_space.n),\n",
    "    n_trajectories=20,\n",
    "    gamma=0.9\n",
    ")\n",
    "print(avg_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpv9bGrwdtFd"
   },
   "source": [
    "We can also visualize the execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k7_UY2RPOb7v"
   },
   "outputs": [],
   "source": [
    "from rlss_practice.wrappers import Renderer\n",
    "visible_env = Renderer(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P2CieU7A_40i",
    "outputId": "e68c4b02-76d2-4bca-b05c-2b83eaaccf8c"
   },
   "outputs": [],
   "source": [
    "avg_return = rollouts(\n",
    "    env=visible_env,\n",
    "    policy=UniformPolicy(env.action_space.n),\n",
    "    n_trajectories=1,\n",
    "    gamma=0.9\n",
    ")\n",
    "print(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "id": "EbdR7lQQXhXu",
    "outputId": "053c62da-6cd9-4ef6-9b28-b81b50f3abb2"
   },
   "outputs": [],
   "source": [
    "visible_env.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srQzHwZTe_gd"
   },
   "source": [
    "### Explicit model\n",
    "\n",
    "These algorithms assume that a complete model of the environment is available, in the form of explicit transition and reward functions. This is not part of the gym interface.\n",
    "\n",
    "These two functions are stored in two members:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DczwaYuUfqHg",
    "outputId": "8eef3250-e601-4ff7-8ccc-6624bc294522"
   },
   "outputs": [],
   "source": [
    "# Transition and reward functions\n",
    "T = env.T\n",
    "R = env.R\n",
    "\n",
    "# These are represented as dictionaries (for maximum clarity)\n",
    "#   and indexed as T[state][action][next_state]\n",
    "print(\"A few probabilities\")\n",
    "print(T[(1, 1, 0)][2][(2, 1, 0)])\n",
    "print(T[(1, 1, 0)][2][(1, 1, 1)])\n",
    "print(T[(3, 1, 0)][1][(3, 1, 1)])\n",
    "\n",
    "print(\"\\nA few rewards\")\n",
    "print(R[(1, 1, 0)][2])\n",
    "print(R[(3, 3, 0)][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lQglr5MOiDCp",
    "outputId": "2df61370-d2f8-42d7-b775-5d5c670aec2b"
   },
   "outputs": [],
   "source": [
    "# Explicit set of states and actions\n",
    "print(\"States\", env.states)\n",
    "print(\"Actions\", env.actions)\n",
    "print(\"Number of states\", len(env.states))\n",
    "print(\"Number of actions\", len(env.actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XfYfq9HebRHw"
   },
   "outputs": [],
   "source": [
    "# Some classic imports for the rest of the notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "\n",
    "mpl.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8L7zlYG8WcPq"
   },
   "source": [
    "## Solving the Grid-world task\n",
    "\n",
    "We would like to find a **stationary deterministic memoryless policy** or mapping from states to actions $\\pi : \\mathcal{S}\\rightarrow \\mathcal{A}$, that lets us arrive at the **goal state** sooner.\n",
    "\n",
    "In other words, our objective to find $\\pi$ which maximizes the **discounted return** from any starting state represented by:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\rho(\\pi) &= \\mathbb{E}_{s_0\\sim\\nu_0, s_{t+1}\\sim p(\\cdot|s_t,\\pi(s_t))}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t,\\pi(s_t))\\right]\\\\\n",
    "    &= \\sum_{s}\\nu_0(s) V^{\\pi}(s)\n",
    "\\end{align*}\n",
    "\n",
    "with the **discount factor** $\\gamma\\in[0,1)$.\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "---\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Recall that the **state value function** for $s\\in \\mathcal{S}$ denoted\n",
    "\n",
    "\\begin{align*}\n",
    "    V^\\pi(s) &= \\mathbb{E}_{s_{t+1}\\sim p(\\cdot|s_t,\\pi(s_t))}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t,\\pi(s_t))\\bigg|s_0=s\\right]\\\\\n",
    "    &=  \\sum_{a}\\pi(a|s)Q^{\\pi}(s,a)\n",
    "\\end{align*}\n",
    "\n",
    "represents the value of being in state $x$ and following policy $\\pi$ while the **action-value function** for $x\\in \\mathcal{S}, a\\in \\mathcal{A}$ denoted\n",
    "\n",
    "$$ Q^\\pi(s,a) = r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V^{\\pi}(s')$$\n",
    "\n",
    "is the value of first taking action $a=\\pi(s)$ in state $s$ then following policy $\\pi$.\n",
    "\n",
    "By standard notation, the **Bellman operator** of policy $\\pi$ acting on functions $V : \\mathcal{S}\\rightarrow \\mathbb{R}$ (or vectors $V\\in\\mathbb{R}^{|\\mathcal{S}|}$) is given as:\n",
    "\n",
    "$$(T^{\\pi}V)(s) =  \\sum_{a}\\pi(a|s)\\bigg[r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V(s')\\bigg],\\quad s\\in\\mathcal{S}$$\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "---\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Ideally, we would like to find an **optimal policy** $\\pi^*$ with:\n",
    "\n",
    "$$ \\pi^*(s) \\in \\underset{a\\in\\mathcal{A}}{\\arg\\max} \\bigg\\{r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V^{\\pi^*}(s')\\bigg\\},\\quad x\\in\\mathcal{S}$$\n",
    "\n",
    "that maximizes our immediate reward and future return.\n",
    "\n",
    "The Bellman operator of $\\pi^*$ (a.k.a the **Bellman optimality operator**) acting on functions $V : \\mathcal{S}\\rightarrow \\mathbb{R}$ (or vectors $V\\in\\mathbb{R}^{|\\mathcal{S}|}$) is given as:\n",
    "\n",
    "$$(T^{*}V)(s) =  \\max_{a}\\bigg\\{r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V(s')\\bigg\\},\\quad s\\in\\mathcal{S}$$\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "---\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "With the transition and reward function of the grid-world task available to us, we attempt to find an optimal policy via **Dynamic programming**. Precisely, we implement **Policy Iteration** and **Value iteration** methods introduced in the first lecture.\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nljnBe4bZTf"
   },
   "source": [
    "### Policy Iteration (PI) Recap\n",
    "\n",
    "**Idea**\n",
    "\n",
    "Gradually advance to $\\pi^{*}$ from an initial guess $\\pi_0$ through a series of **policy evaluation** and **policy improvement** steps.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Policy Evaluation step**\n",
    "\n",
    "Given a policy $\\pi_k$, compute $V^{\\pi_k}$ as\n",
    "\n",
    "* $V^{\\pi_k} = (I - \\gamma\\, p_{\\pi_k})^{-1}r_{\\pi_k}$\n",
    "\n",
    "or estimate $V^{\\pi_k}$ as $V_{k}$ with the following iterations\n",
    "\n",
    "  * Initialize $V$ as $\\mathbf{0}$ when $k = 0$ and $V_{k-1}$ otherwise, let $\\pi = \\pi_k$\n",
    "\n",
    "    <div>\n",
    "    <img src=\"https://drive.google.com/uc?export=view&id=1QaMg7a6HELjYycAnm6RE9vnzHD_fnaCn\" alt=\"iterative policy evaluation\" width=\"600\"/>\n",
    "    </div>\n",
    "\n",
    "    Return $V_k = V$. <!--$(T^{\\pi_{k}})^{n_{k}}V_{k-1}$-->\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Policy Improvement step**\n",
    "\n",
    "Obtain $\\pi_{k+1}$ as the greedy policy w.r.t $V^{\\pi_k}$ (or $V_{k}$). That is,\n",
    "$$ \\pi_{k+1}(s) \\in \\underset{a\\in\\mathcal{A}}{\\arg\\max} \\bigg\\{r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{X}}p(s'|s,a)V^{\\pi_{k}}(s')\\bigg\\},\\quad s\\in\\mathcal{S}$$\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "#### Putting everything together\n",
    "\n",
    "Starting from an arbitrary stationary deterministic markovian policy $\\pi_{0}$, for $k = 0,1,2,\\cdots, K$ do:\n",
    "* Compute $V^{\\pi_k}$ or estimate $V^{\\pi_k}$ with $V_{k}=(T^{\\pi_{k}})^{n_{k}}V_{k-1}$\n",
    "* Obtain $\\pi_{k+1}$ as the greedy policy w.r.t $V^{\\pi_k}$\n",
    "\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "#### Theoretical guarantee\n",
    "\n",
    "PI with \"exact\" policy evaluation steps finds an optimal policy after $K = \\mathcal{\\tilde{O}}((\\mathcal{SA - S})/(1-\\gamma))$ iterations.\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "#### Notation\n",
    "\n",
    "*   $r_{\\pi}\\in\\mathbb{R}^{|\\mathcal{S}|}$ so that for $s\\in\\mathcal{S}$, $r_{\\pi}(s) = r(s,\\pi(s))$.\n",
    "*   $p_{\\pi}\\in\\mathbb{R}^{|\\mathcal{S}|\\times|\\mathcal{S}|}$ so that for $s,s'\\in\\mathcal{S}$, $p_{\\pi}(s'|s) = p(s'|s,\\pi(s))$.\n",
    "*   $n_{k}$ is the number of loops required to compute $V_{k}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Woh49RAfbieB"
   },
   "outputs": [],
   "source": [
    "class PolicyIteration1:\n",
    "  \"\"\"\n",
    "  Implements policy iteration with exact policy evaluation\n",
    "  \"\"\"\n",
    "  def __init__(self, env: Env, discount_factor: float, initial_policy = None):\n",
    "    # Store\n",
    "    self.env = env\n",
    "    self.states = self.env.states\n",
    "    self.n_states = len(self.states)\n",
    "    self.actions = self.env.actions\n",
    "    self.n_actions = len(self.actions)\n",
    "    self.T = self.env.T\n",
    "    self.R = self.env.R\n",
    "    self.gamma = discount_factor\n",
    "    self.policy = initial_policy\n",
    "\n",
    "    # Default policy\n",
    "    if self.policy == None:\n",
    "      np.random.seed(4)\n",
    "      self.policy = {state: np.random.randint(0, self.n_actions-1) for state in self.states}\n",
    "\n",
    "    self.V = np.zeros((self.n_states,1))\n",
    "    self.policy_stable = False\n",
    "    self.V_logs = []\n",
    "\n",
    "\n",
    "  def evaluate_policy(self):\n",
    "    \"\"\"\n",
    "    Given π_{k} compute V^{π_{k}} = (I - \\gamma p_{π_{k}})^{-1}r_{π_{k}}.\n",
    "    \"\"\"\n",
    "    # Value\n",
    "    A = np.eye(self.n_states,self.n_states) - self.gamma*self.get_p_pi()\n",
    "    b = self.get_r_pi()\n",
    "    self.V = np.linalg.solve(A, b)\n",
    "\n",
    "    # Log\n",
    "    self.V_logs.append({state: self.V[i].item() for i, state in enumerate(self.states)})\n",
    "    return\n",
    "\n",
    "\n",
    "  def get_policy(self):\n",
    "    \"\"\"\n",
    "    Compute the greedy policy:\n",
    "      π_{k+1}(s) = argmax_{a\\in A} Q^{\\pi_{k}}(s,a)\n",
    "    where\n",
    "      Q^{\\pi_{k}}(s,a) = R(s,a) + gamma*<P(.|s,a),V^{\\pi_{k}}>\n",
    "    is the state-action value.\n",
    "    \"\"\"\n",
    "    self.policy_stable = True\n",
    "\n",
    "    for state in self.states:\n",
    "      max_Q_value = self.get_expected_update(state, self.policy[state])\n",
    "\n",
    "      for action in self.actions:\n",
    "        Q_value = self.get_expected_update(state, action)\n",
    "\n",
    "        if action != self.policy[state] and max_Q_value < Q_value:\n",
    "          self.policy[state] = action\n",
    "          max_Q_value = Q_value\n",
    "          self.policy_stable = False\n",
    "    return\n",
    "\n",
    "\n",
    "  def get_expected_update(self, state, action):\n",
    "    \"\"\"\n",
    "    Compute Bellman update at a state-action pair.\n",
    "\n",
    "    input:\n",
    "    state\n",
    "    action\n",
    "    discount factor (gamma)\n",
    "    state values (state_values)\n",
    "\n",
    "    output:\n",
    "    r(s,a) + gamma <P(.|s,a),v>\n",
    "    \"\"\"\n",
    "    value  = self.R[state][action]\n",
    "\n",
    "    for snext_index, snext in enumerate(self.states):\n",
    "      value += self.gamma * self.T[state][action][snext] * self.V[snext_index]\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "  def get_p_pi(self):\n",
    "    \"\"\"\n",
    "    Given π_{k}, compute p_{π_{k}}\n",
    "    \"\"\"\n",
    "    p_pi = np.zeros((self.n_states,self.n_states))\n",
    "\n",
    "    for s_index, s in enumerate(self.states):\n",
    "      for snext_index, snext in enumerate(self.states):\n",
    "        p_pi[s_index, snext_index] = self.T[s][self.policy[s]][snext]\n",
    "\n",
    "    return p_pi\n",
    "\n",
    "\n",
    "  def get_r_pi(self):\n",
    "    \"\"\"\n",
    "    Given π_{k}, compute r_{π_{k}}\n",
    "    \"\"\"\n",
    "    r_pi = np.zeros((self.n_states,1))\n",
    "\n",
    "    for i, state in enumerate(self.states):\n",
    "      r_pi[i][0] = self.R[state][self.policy[state]]\n",
    "\n",
    "    return r_pi\n",
    "\n",
    "\n",
    "  def as_policy(self):\n",
    "    \"\"\"Return the policy dictionary as a callable\"\"\"\n",
    "    def callable_policy(observation):\n",
    "        return self.policy[tuple(observation.tolist())]\n",
    "    return callable_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFkMVMI3syvt"
   },
   "outputs": [],
   "source": [
    "class PolicyIteration2(PolicyIteration1):\n",
    "  \"\"\"\n",
    "  Implements policy iteration with iterative policy evaluation\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "              env: Env,\n",
    "              discount_factor: float,\n",
    "              theta: float,\n",
    "              initial_policy = None):\n",
    "\n",
    "    super().__init__(env, discount_factor, initial_policy)\n",
    "    self.theta = theta\n",
    "    self.V = {state: 0 for state in env.states}\n",
    "\n",
    "\n",
    "  def evaluate_policy(self):\n",
    "    \"\"\"\n",
    "    Starting from previous value estimate V_{k-1}\n",
    "    estimate value of policy π_{k} by recursively applying\n",
    "    the Bellman operator of the policy T^π_{k} to V_{k-1} until update is stable.\n",
    "    \"\"\"\n",
    "    max_value_gap = np.inf\n",
    "\n",
    "    while max_value_gap > self.theta:\n",
    "      max_value_gap = 0\n",
    "\n",
    "      for state in env.states:\n",
    "        prev_statevalue = self.V[state]\n",
    "        self.V[state] = self.get_expected_update(state, self.policy[state])\n",
    "        max_value_gap = max(max_value_gap, np.abs(prev_statevalue - self.V[state]))\n",
    "\n",
    "    self.V_logs.append(self.V.copy())\n",
    "\n",
    "\n",
    "  def get_expected_update(self, state, action):\n",
    "    \"\"\"\n",
    "    Compute Bellman update at a state-action pair.\n",
    "\n",
    "    input:\n",
    "    state\n",
    "    action\n",
    "    discount factor (gamma)\n",
    "    state values (state_values)\n",
    "\n",
    "    output:\n",
    "    r(s,a) + gamma <P(.|s,a),v>\n",
    "    \"\"\"\n",
    "    value  = env.R[state][action]\n",
    "\n",
    "    for snext in env.states:\n",
    "      value += self.gamma*env.T[state][action][snext] * self.V[snext]\n",
    "\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IA8AkX2Feuwt"
   },
   "source": [
    "### Value Iteration (VI) Recap\n",
    "\n",
    "**Idea**\n",
    "\n",
    "Gradually advance to $\\pi^{*}$ with a single **iterative policy evaluation** step and **policy improvement**.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Policy Evaluation step**\n",
    "\n",
    "No arbitrary starting policy needed. Estimate $V^{\\pi^*}$ as follows:\n",
    "\n",
    "  * For $k = 0,1,2,\\cdots, K$\n",
    "\n",
    "    * Set $V = \\mathbf{0}$ when $k = 0$ and $V_{k-1}$ otherwise\n",
    "      <div>\n",
    "      <img src=\"https://drive.google.com/uc?export=view&id=1fsi3ZZgqZ-p061AxvSdeluWZTlwSjnGj\" alt=\"iterative policy evaluation\" width=\"400\"/>\n",
    "      </div>\n",
    "\n",
    "      return $V$ as $V_{k}$.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Policy Improvement step**\n",
    "\n",
    "Obtain $\\hat{\\pi}^{*}$ as the greedy policy w.r.t $V_{K}$. That is,\n",
    "\n",
    "  $$ \\hat{\\pi}^{*}(s) \\in \\underset{a\\in\\mathcal{A}}{\\arg\\max} \\bigg\\{r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{X}}p(s'|s,a)V_{K}(s')\\bigg\\},\\quad s\\in\\mathcal{S}$$\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "#### Putting everything together\n",
    "\n",
    "* Estimate $V^{\\pi^*}$ with $V_{K} = (T^*)^{K}\\mathbf{0}$\n",
    "* Obtain $\\hat{\\pi}^{*}$ as the greedy policy w.r.t $V_{K}$\n",
    "\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "#### Theoretical guarantee\n",
    "\n",
    "VI finds an **$\\mathbf{\\varepsilon}$-optimal policy** ($\\pi^{\\varepsilon}$) satisfying\n",
    "\n",
    "$\\qquad V^* - V^{\\pi^{\\varepsilon}}\\leq \\varepsilon\\, e$\n",
    "\n",
    "after $K = \\mathcal{O}(\\ln(2\\gamma/\\varepsilon(1-\\gamma)^2)/(1-\\gamma))$ iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cWtqSTiD9Kko"
   },
   "outputs": [],
   "source": [
    "class ValueIteration:\n",
    "    \"\"\"\n",
    "    Implements value iteration\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                discount_factor: float,\n",
    "                epsilon: float,\n",
    "                num_iterations = None,\n",
    "    ):\n",
    "      self.gamma = discount_factor\n",
    "      self.K = num_iterations\n",
    "      self.V_logs = []\n",
    "\n",
    "      if self.K is None:\n",
    "        self.K = math.ceil(np.log((2*self.gamma)/(epsilon*(1-self.gamma)**2))/(1-self.gamma))\n",
    "\n",
    "      self.policy = {state: 0 for state in env.states}\n",
    "      self.V = {state: 0 for state in env.states}\n",
    "\n",
    "\n",
    "    def evaluate_policy(self):\n",
    "      \"\"\"\n",
    "      Starting from an arbitrary value V_0(s) = 0 for s in S\n",
    "      estimate the V^π* by recursively applying\n",
    "      the Bellman optimality operator of the policy T^* to V_0 for K steps.\n",
    "      \"\"\"\n",
    "\n",
    "      for k in range(self.K):\n",
    "        for state in env.states:\n",
    "          self.V[state] = max(map(lambda action: self.get_expected_update(state, action), env.actions))\n",
    "\n",
    "      self.V_logs.append(self.V.copy())\n",
    "\n",
    "\n",
    "    def get_policy(self):\n",
    "      \"\"\"\n",
    "      Update policy to be the greedy policy:\n",
    "        π(s) = argmax_{a\\in A} Q(s,a)\n",
    "      where\n",
    "        Q(s,a) = R(s,a) + gamma*<P(.|s,a),v>\n",
    "      is the state-action value.\n",
    "      \"\"\"\n",
    "      for state in env.states:\n",
    "        max_Q_value = self.get_expected_update(state, self.policy[state])\n",
    "        for action in env.actions:\n",
    "          Q_value = self.get_expected_update(state, action)\n",
    "          if action != self.policy[state] and max_Q_value < Q_value:\n",
    "            self.policy[state] = action\n",
    "            max_Q_value = Q_value\n",
    "      return\n",
    "\n",
    "\n",
    "    def get_expected_update(self, state, action):\n",
    "      \"\"\"\n",
    "      Compute Bellman update at a state-action pair.\n",
    "\n",
    "      input:\n",
    "      state\n",
    "      action\n",
    "      discount factor (gamma)\n",
    "      state values (state_values)\n",
    "\n",
    "      output:\n",
    "      r(s,a) + gamma <P(.|s,a),v>\n",
    "      \"\"\"\n",
    "      value  = env.R[state][action]\n",
    "\n",
    "      for snext in env.states:\n",
    "        value += self.gamma * env.T[state][action][snext] * self.V[snext]\n",
    "\n",
    "      return value\n",
    "\n",
    "    def as_policy(self):\n",
    "      \"\"\"Return the policy dictionary as a callable\"\"\"\n",
    "      def callable_policy(observation):\n",
    "          return self.policy[tuple(observation.tolist())]\n",
    "      return callable_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nuH15i1W4qr"
   },
   "source": [
    "### Run and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yyFqB2DG5Alo"
   },
   "outputs": [],
   "source": [
    "discount_factor = 0.9\n",
    "epsilon = 0.01\n",
    "\n",
    "# Run iteration with exact policy evaluation step\n",
    "PI_planner1 = PolicyIteration1(env, discount_factor)\n",
    "\n",
    "while not PI_planner1.policy_stable:\n",
    "  PI_planner1.evaluate_policy()\n",
    "  PI_planner1.get_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQkER_MsJiCK"
   },
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6btXwfrRJev2"
   },
   "outputs": [],
   "source": [
    "def visualize(vlogs, state, grid_size):\n",
    "  grid_size -= 2   # walls\n",
    "\n",
    "  # Initial state values\n",
    "  fig1 = plt.figure(figsize=(4, 3))\n",
    "  ax1 = fig1.subplots()\n",
    "  ax1.set_ylabel(\"value of (1,1,0)\")\n",
    "  ax1.set_xlabel(\"iterations\")\n",
    "  initial_state_values = [value[(1, 1, 0)] for value in PI_planner1.V_logs]\n",
    "  ax1.plot(range(len(initial_state_values)), initial_state_values)\n",
    "\n",
    "  # States values over time\n",
    "  some_values_over_time = [\n",
    "    {(x, y): values[(x, y, o)] for (x, y, o) in env.states if o == 0}  # o is a fixed agent orientation\n",
    "    for values in vlogs\n",
    "  ]\n",
    "  values_over_time = [\n",
    "    np.array([[values[(x+1, y+1)] for x in range(grid_size)] for y in range(grid_size)])\n",
    "    for values in some_values_over_time\n",
    "  ]\n",
    "  vmids = [(values.max() + values.min()) / 2 for values in values_over_time]\n",
    "  vmin = min([values.min() for values in values_over_time])\n",
    "  vmax = max([values.max() for values in values_over_time])\n",
    "\n",
    "  steps = len(values_over_time)\n",
    "  fig2 = plt.figure(figsize=(4 * steps, 3))\n",
    "  axs2 = fig2.subplots(1, steps)\n",
    "  if isinstance(axs2, plt.Axes):\n",
    "    axs2 = [axs2]\n",
    "    multistep = False\n",
    "  else:\n",
    "    multistep = True\n",
    "\n",
    "  for i, ax in enumerate(axs2):\n",
    "    ax.set_title(f\"values at step {i}\" if multistep else \"values\")\n",
    "    ax.imshow(values_over_time[i], cmap=\"Blues\", vmin=vmin, vmax=vmax)\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.yaxis.set_visible(False)\n",
    "    for x in range(grid_size):\n",
    "      for y in range(grid_size):\n",
    "        val = values_over_time[i][y,x]\n",
    "        ax.text(x, y, f\"{val:.2f}\", ha=\"center\", va=\"center\", color=\"w\" if val > vmids[i] else \"k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 583
    },
    "id": "XvCmq5bQQqEi",
    "outputId": "8854a9c6-eefb-403c-e390-3ef9f01aafdd"
   },
   "outputs": [],
   "source": [
    "visualize(PI_planner1.V_logs, (1, 1, 0), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meYQb9VOTnDT"
   },
   "source": [
    "We can also test the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3C-WmCiVT6op",
    "outputId": "b1faec58-3c4f-4603-861a-adb30a650bda"
   },
   "outputs": [],
   "source": [
    "avg_return = rollouts(\n",
    "    env=visible_env,\n",
    "    policy=PI_planner1.as_policy(),\n",
    "    n_trajectories=20,\n",
    "    gamma=0.9\n",
    ")\n",
    "print(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "id": "LGxMz5yZEZ-N",
    "outputId": "e1c642aa-0e84-4e57-f15e-9b6140b25a8a"
   },
   "outputs": [],
   "source": [
    "visible_env.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymYjjFBYbiy-"
   },
   "source": [
    "What if we have failure probabilities now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8EcH0mS2bt1N",
    "outputId": "b9cb6028-2e0d-485c-ef88-c1fb59977aeb"
   },
   "outputs": [],
   "source": [
    "# Define\n",
    "env = Room(\n",
    "    failure=0.2,\n",
    "    agent_start_pos=(1, 1),\n",
    "    agent_start_dir=1,\n",
    "    size=6,\n",
    ")\n",
    "visible_env = Renderer(env)\n",
    "print(env)\n",
    "\n",
    "# Plan\n",
    "PI_planner1 = PolicyIteration1(env, discount_factor)\n",
    "while not PI_planner1.policy_stable:\n",
    "  PI_planner1.evaluate_policy()\n",
    "  PI_planner1.get_policy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gvuqeY88cCW2",
    "outputId": "e98f8964-73c9-4048-e81f-74d460dd0f1e"
   },
   "outputs": [],
   "source": [
    "# Visualize\n",
    "avg_return = rollouts(\n",
    "    env=visible_env,\n",
    "    policy=PI_planner1.as_policy(),\n",
    "    n_trajectories=20,\n",
    "    gamma=0.9\n",
    ")\n",
    "print(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 779
    },
    "id": "I-hHn_KXcKaS",
    "outputId": "efed5cdb-19b7-470c-9191-7c265eb7794e"
   },
   "outputs": [],
   "source": [
    "visible_env.play()\n",
    "\n",
    "visualize(PI_planner1.V_logs, (1, 1, 1), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KofhGOYGhckf"
   },
   "source": [
    "Now we run policy iteration with the approximate policy evaluation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9uBUwD-hhSF"
   },
   "outputs": [],
   "source": [
    "theta = 0.1\n",
    "PI_planner2 = PolicyIteration2(env, discount_factor, theta)\n",
    "\n",
    "while not PI_planner2.policy_stable:\n",
    "  PI_planner2.evaluate_policy()\n",
    "  PI_planner2.get_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70ieiSAZqR6H",
    "outputId": "4bbb3af8-6ea9-4285-9771-88844bbd0f76"
   },
   "outputs": [],
   "source": [
    "avg_return = rollouts(\n",
    "    env=visible_env,\n",
    "    policy=PI_planner2.as_policy(),\n",
    "    n_trajectories=10,\n",
    "    gamma=0.9\n",
    ")\n",
    "print(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 779
    },
    "id": "cCueLRiiqvaQ",
    "outputId": "00c1318c-454c-4157-a31f-ea1af36e4f62"
   },
   "outputs": [],
   "source": [
    "visible_env.play()\n",
    "\n",
    "visualize(PI_planner2.V_logs, (1, 1, 1), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7DXuWm3v82W"
   },
   "source": [
    "Finally, we also plan with Value Iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BFFnzxCfaS-E"
   },
   "outputs": [],
   "source": [
    "VI_planner = ValueIteration(discount_factor, epsilon)\n",
    "\n",
    "VI_planner.evaluate_policy()\n",
    "VI_planner.get_policy()\n",
    "policy = VI_planner.as_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dgj9qrYohLWm",
    "outputId": "2695138b-6690-48bb-e071-7d5be6123187"
   },
   "outputs": [],
   "source": [
    "avg_return = rollouts(\n",
    "    env=visible_env,\n",
    "    policy=VI_planner.as_policy(),\n",
    "    n_trajectories=20,\n",
    "    gamma=0.9\n",
    ")\n",
    "print(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 779
    },
    "id": "QY-Vrp5J0gDC",
    "outputId": "9ccc267d-55cd-4cd1-f21e-353739d6a560"
   },
   "outputs": [],
   "source": [
    "visible_env.play()\n",
    "\n",
    "visualize(VI_planner.V_logs, (1, 1, 0), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqJPFe6f6gFV"
   },
   "source": [
    "Credit:\n",
    "\n",
    "*   Bruno Scherrer, \"a lecture on Markov Decision Processes and Dynamic Programming\", June 2023, [link text](https://)\n",
    "*   Csaba Szepesvári \"a lecture series on Theoretical Foundations of Reinforcement Learning\", 2020, [RL Theory course](https://rltheory.github.io/)\n",
    "*   Richard S. Sutton, Andrew G. Barto \"Reinforcement Learning: An Introduction\", second edition, 2020, [Book](http://incompleteideas.net/book/RLbook2020.pdf)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
