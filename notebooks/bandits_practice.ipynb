{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91ec4dea",
   "metadata": {},
   "source": [
    "Using code from Claire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5983e3b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'display'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-db440591117f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrlss_practice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrlss_practice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_regret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/practical-sessions/rlss_practice/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrlss_practice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_board\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'display'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys \n",
    "sys.path.insert(0, '..')\n",
    "from rlss_practice.model import Environment, Agent\n",
    "from rlss_practice.display import plot_regret\n",
    "\n",
    "from scipy.stats import bernoulli\n",
    "from math import log\n",
    "\n",
    "import random\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c68647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(environment, agent, Nmc, T):\n",
    "    \"\"\"\n",
    "    Play one Nmc trajectories over a horizon T for the specified agent. \n",
    "    Return the agent's name (sring) and the collected data in an nd-array.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = np.zeros((Nmc, T))\n",
    "    \n",
    "    for n in range(Nmc):\n",
    "        agent.reset()\n",
    "        #action_set = environment.get_action_set()\n",
    "        for t in range(T):\n",
    "            action = agent.get_action() \n",
    "            # Note that the main difference with the previous lab is that now get_action needs to receive the action_set\n",
    "            reward = environment.get_reward(action)\n",
    "            agent.receive_reward(action,reward)\n",
    "            # compute instant regret\n",
    "            means = environment.get_means()\n",
    "            best_reward = np.max(means)\n",
    "            data[n,t]= best_reward - reward # this can be negative due to the noise, but on average it's positive\n",
    "            \n",
    "    return agent.name(), data\n",
    "\n",
    "\n",
    "def experiment(environment, agents, Nmc, T):\n",
    "    \"\"\"\n",
    "    Play Nmc trajectories for all agents over a horizon T. Store all the data in a dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_data = {}\n",
    "    \n",
    "    for agent in agents:\n",
    "        agent_id, regrets = play(environment, agent,Nmc, T)\n",
    "        \n",
    "        all_data[agent_id] = regrets\n",
    "        \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cb0892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomActionsGenerator(K,d, mean=None):\n",
    "    return np.diag(np.full(d,1)) #vecs / norms[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32d7bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IID_env(Environment):\n",
    "    \n",
    "    def __init__(self, theta, K):\n",
    "        \"\"\"\n",
    "        theta: d-dimensional vector (bounded) representing the hidden parameter\n",
    "        K: number of actions per round (random action vectors generated each time)\n",
    "        \"\"\"\n",
    "        self.d = np.size(theta)\n",
    "        self.theta = theta\n",
    "        self.K = K\n",
    "\n",
    "            \n",
    "        \n",
    "    def get_reward(self, action):\n",
    "    \n",
    "        \"\"\" sample reward given action \n",
    "        \"\"\"\n",
    "\n",
    "        return np.random.normal(self.theta[action])\n",
    "            \n",
    "    def get_means(self):\n",
    "        return  self.theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b22f50",
   "metadata": {},
   "source": [
    " # Environment Class\n",
    "\n",
    "The environment class allows to create 3 types of linear bandit problems: \n",
    "* 'fixed' : normally requires a fixed_actions input (otherwise randomly generated at start) which is kept all along the game;\n",
    "* 'arbitrary': at each round, an 'arbitrary' set of actions is chosen and here we decided to simply create a pool of (d x K) vectors and let the environment choose K of them without replacement at each round;\n",
    "* 'iid' : at each round, the environment samples K actions at random on the sphere.\n",
    "\n",
    "For each of these types of game, the class is used to generate the action sets at each round and the reward for a chosen action (chosen by an Agent, see the \"Play!\" section for the details of the interaction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddbace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy(Agent):\n",
    "  def __init__(self, d, eps=0.1):\n",
    "    self.eps = eps # exploration probability\n",
    "    self.d = d\n",
    "    self.reset()\n",
    "    \n",
    "    self.cumulative_reward = np.zeros(self.d)\n",
    "    self.num_played = np.zeros(self.d)\n",
    "    self.hat_theta = np.zeros(self.d)\n",
    "\n",
    "    \n",
    "  def reset(self):\n",
    "    self.t = 0\n",
    "    self.hat_theta = np.zeros(self.d)\n",
    "\n",
    "\n",
    "  def get_action(self):\n",
    "    \n",
    "    u = np.random.random()\n",
    "    if u<self.eps:\n",
    "        return np.random.choice(self.d)\n",
    "    else:\n",
    "        chosen_arm_index = np.argmax(self.hat_theta)\n",
    "        return chosen_arm_index\n",
    "\n",
    "  def receive_reward(self, chosen_arm, reward):\n",
    "    \"\"\"\n",
    "    update the internal quantities required to estimate the parameter theta using least squares\n",
    "    \"\"\"\n",
    "\n",
    "    self.cumulative_reward[chosen_arm] += reward # update\n",
    "    self.num_played[chosen_arm] += 1\n",
    "    self.hat_theta[chosen_arm] = self.cumulative_reward[chosen_arm]/self.num_played[chosen_arm] # update\n",
    "\n",
    "    self.t += 1   \n",
    "        \n",
    "\n",
    "  #@staticmethod\n",
    "  def name(self):\n",
    "    return 'EGreedy('+str(self.eps)+')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078d3a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 3  # dimension\n",
    "K = 7  # number of arms\n",
    "\n",
    "# parametor vector \\theta, normalized :\n",
    "# theta = randomActionsGenerator(1,d)\n",
    "theta = np.array([0.1, 0.3, 0.9])\n",
    "\n",
    "T = 1000  # Horizon\n",
    "N = 50  # Monte Carlo simulations\n",
    "\n",
    "delta = 0.05 # could be set directly in the algorithms\n",
    "\n",
    "# save subsampled points for Figures\n",
    "Nsub = 100\n",
    "tsav = range(2, T, Nsub)\n",
    "\n",
    "#choice of percentile display\n",
    "q = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901d779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "iid_env = IID_env(theta, d) \n",
    "e_greedy = EpsilonGreedy(d,  eps=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513e97e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_greedy_experiment = experiment(iid_env, [e_greedy], Nmc=N, T=T)\n",
    "plot_regret(e_greedy_experiment, q=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ea1344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bbb662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
